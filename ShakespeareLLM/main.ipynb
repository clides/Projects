{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "19351064-78b2-4e8f-847f-c68ee08ac8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the stuff\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8889c668-fda1-46db-87ab-c3736aee4bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65376ce2-74e0-4e64-9dbc-e89232deeafd",
   "metadata": {},
   "source": [
    "# Creating training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d021ea06-ecaf-490b-a228-208bf40e5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'shakespeare.txt'\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# storing the text in a variable\n",
    "text = open(filepath, 'rb').read().decode(encoding='utf-8').lower()\n",
    "\n",
    "# selecting only a portion of the text because the model takes a long time to train\n",
    "text = text[300000:800000]\n",
    "\n",
    "# creating a mapping from unique characters to integers\n",
    "chars = sorted(set(text)) # set returns unique characters in text\n",
    "\n",
    "# creating a dictionary for the mapping\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# filling up the sentences and the next characters lists\n",
    "SEQ_LENGTH = 40\n",
    "STEP_SIZE = 3\n",
    "\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - SEQ_LENGTH, STEP_SIZE):\n",
    "    sentences.append(text[i: i+SEQ_LENGTH])\n",
    "    next_chars.append(text[i+SEQ_LENGTH])\n",
    "    \n",
    "# declaring np arrays for input and output\n",
    "X = np.zeros(shape=(len(sentences), SEQ_LENGTH, len(chars)), dtype=np.float32)\n",
    "y = np.zeros(shape=(len(sentences), len(chars)), dtype=np.float32)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be7069b2-0923-417b-9fa8-9b2fea729133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating custom datasets and splitting it into batches\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06efe4ec-5f5d-4bd8-9f61-08fbd4a88548",
   "metadata": {},
   "source": [
    "# Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5d76f10a-7890-4752-afd1-aab9fd1cdc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a wrapper class because LSTM layer outputs a tuple and Linear can only take in a tensor\n",
    "class LSTMWrapper(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, hidden_size):\n",
    "        super(LSTMWrapper, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, num_layers=num_layers, hidden_size=hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_output, _ = self.lstm(x)  # Only return the `output`\n",
    "        return lstm_output[:, -1, :]  # Return the last time step's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "dadbc3ca-63fd-4f07-8786-fbd073b5817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model\n",
    "model = nn.Sequential(\n",
    "    LSTMWrapper(input_size=len(chars), num_layers=1, hidden_size=128),\n",
    "    nn.Linear(in_features=128, out_features=len(chars)),\n",
    "    # nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# creating the loss function and the optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00987b1-b496-48b7-84b7-6c1208568f2e",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b4ddee8d-2e52-4237-a7fe-2becbfd37c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that trains the model\n",
    "def train_model(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        total_batches = len(train_loader)\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader, start=1):\n",
    "            # Convert one-hot encoded targets to class indices\n",
    "            targets = torch.argmax(targets, dim=1)\n",
    "            \n",
    "            y_pred = model(inputs)\n",
    "            loss = loss_fn(y_pred, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if (batch_idx) % 100 == 0:\n",
    "                print(f\"Epoch: {epoch+1}/{epochs} | Batch: {batch_idx+1}/{total_batches} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0b88b2f0-7d3e-497b-8ad4-dd0077da8152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 | Batch: 101/489 | Loss: 3.2593\n",
      "Epoch: 1/5 | Batch: 201/489 | Loss: 3.0165\n",
      "Epoch: 1/5 | Batch: 301/489 | Loss: 2.7164\n",
      "Epoch: 1/5 | Batch: 401/489 | Loss: 2.4861\n",
      "Epoch: 2/5 | Batch: 101/489 | Loss: 2.5716\n",
      "Epoch: 2/5 | Batch: 201/489 | Loss: 2.3819\n",
      "Epoch: 2/5 | Batch: 301/489 | Loss: 2.3319\n",
      "Epoch: 2/5 | Batch: 401/489 | Loss: 2.3309\n",
      "Epoch: 3/5 | Batch: 101/489 | Loss: 2.2545\n",
      "Epoch: 3/5 | Batch: 201/489 | Loss: 2.1641\n",
      "Epoch: 3/5 | Batch: 301/489 | Loss: 2.2286\n",
      "Epoch: 3/5 | Batch: 401/489 | Loss: 2.0915\n",
      "Epoch: 4/5 | Batch: 101/489 | Loss: 1.9915\n",
      "Epoch: 4/5 | Batch: 201/489 | Loss: 2.2164\n",
      "Epoch: 4/5 | Batch: 301/489 | Loss: 1.9649\n",
      "Epoch: 4/5 | Batch: 401/489 | Loss: 2.1249\n",
      "Epoch: 5/5 | Batch: 101/489 | Loss: 2.0499\n",
      "Epoch: 5/5 | Batch: 201/489 | Loss: 2.0009\n",
      "Epoch: 5/5 | Batch: 301/489 | Loss: 2.0494\n",
      "Epoch: 5/5 | Batch: 401/489 | Loss: 1.9146\n"
     ]
    }
   ],
   "source": [
    "train_model(epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2439171d-4b3f-4530-9f89-090999576e8d",
   "metadata": {},
   "source": [
    "## Saving/Loading/Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2e7ae6b7-e061-408c-88ac-64d60cc20a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that saves the model\n",
    "def save_model(model_name : str):\n",
    "    # create model path directory\n",
    "    MODEL_PATH = Path(\"models\")\n",
    "    MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # create model save path\n",
    "    MODEL_NAME = model_name + \".pth\"\n",
    "    MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "    \n",
    "    # save the model state_dict\n",
    "    torch.save(obj=model.state_dict(), f=MODEL_SAVE_PATH)\n",
    "\n",
    "\n",
    "# function that loads model\n",
    "def load_model(MODEL_SAVE_PATH):\n",
    "    model_loaded = model\n",
    "    model_loaded.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "    return model_loaded\n",
    "\n",
    "\n",
    "# function that evaluates the model on the test set\n",
    "def eval_model(model):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, targets in test_loader:\n",
    "            # Forward pass: Compute predictions\n",
    "            outputs = model_1(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    # Print average test loss\n",
    "    print(f\"Test Loss: {total_loss / len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c6ff70d3-2384-421f-b0f3-02188fd35029",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(\"model_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "388acf7f-475b-457a-8e95-b666c6593129",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Sequential:\n\tUnexpected key(s) in state_dict: \"0.lstm.weight_ih_l1\", \"0.lstm.weight_hh_l1\", \"0.lstm.bias_ih_l1\", \"0.lstm.bias_hh_l1\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_v1 \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/model_v1.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[130], line 18\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(MODEL_SAVE_PATH)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(MODEL_SAVE_PATH):\n\u001b[1;32m     17\u001b[0m     model_loaded \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mmodel_loaded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_SAVE_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_loaded\n",
      "File \u001b[0;32m~/virtualenvs/deeplearning/lib/python3.11/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Sequential:\n\tUnexpected key(s) in state_dict: \"0.lstm.weight_ih_l1\", \"0.lstm.weight_hh_l1\", \"0.lstm.bias_ih_l1\", \"0.lstm.bias_hh_l1\". "
     ]
    }
   ],
   "source": [
    "model_v1 = load_model('models/model_v1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9b8b671a-1f61-4f05-b9ff-99289d58c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v2 = load_model('models/model_v2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1292bf-80d9-422d-88c1-12bfd5184d1d",
   "metadata": {},
   "source": [
    "## Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fa6bb1c8-fbfd-44eb-9475-d68d67f838c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    \n",
    "    # Clip predictions to avoid log(0)\n",
    "    preds = np.clip(preds, 1e-8, None)\n",
    "    \n",
    "    # Apply temperature scaling\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    \n",
    "    # Normalize to get probabilities\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    \n",
    "    # Sample from the probability distribution\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "406ca883-50d5-4b93-8e09-8aaa2d465994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, length, temperature):\n",
    "    start_index = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + SEQ_LENGTH]\n",
    "    generated += sentence\n",
    "\n",
    "    for i in range(length):\n",
    "        x = np.zeros((1, SEQ_LENGTH, len(chars)))\n",
    "        for t, character in enumerate(sentence):\n",
    "            x[0, t, char_to_int[character]] = 1\n",
    "\n",
    "        # Convert NumPy array to a PyTorch tensor\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "        predictions = model(x_tensor)[0].detach().numpy()\n",
    "        next_index = sample(predictions, temperature)\n",
    "        next_char = int_to_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "09e1e90b-bc69-4d6e-aad3-d01ff443d0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ldiers, ere her native king\n",
      "shall falteroaoooaoooiodydlyyyyly lyyyyyyyyylyylyyyyllyyyyyyyyyyyyyyyyyyyyyyyryylyyyyylyyyyyyyyryyyrrlayyyyyyyyyy.yyyyylyyyyyyyyyyyyyyyyryylyyyyyyyyyyyyrryayyyyyyyyylyylylyyyyyyyyyyyylyyyylryyyyyyyyyylyyylyyylyyyyyyyylyyyyrrlyyyyrlyyyyyyyylyylyyyyyyyyyyyyyyyyryyyyyyylyyylyyyyrlyyyyyylyyyrlyyyyyyyyyylyyyyyyylyyy\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model_v1, 300, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0c93d890-cff0-4295-acd2-62a659860a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ocence; we knew not\n",
      "the doctrine of ill-sing this to my leall and hor. be i to mand be bot with a payen the dead to sords in the fare.\n",
      "\n",
      "juliten: and be dout and bath at sear thinger wath the ponce fares and i to to butter thou heving his do\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model_v2, 200, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b9ce8-30bc-4893-992b-8d2df2ed10ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
